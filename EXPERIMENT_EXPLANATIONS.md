# Объяснение результатов эксперимента

## 1. Измерение памяти (360-390 МБ)

**Замечание:** "Память слишком ровно у всех, похоже мерял память всего процесса"

**Объяснение:**
- Память измеряется через `psutil.Process(os.getpid())` для **всего процесса Python**
- Это корректный подход для сравнения, так как:
  - Каждый фреймворк запускается в том же процессе
  - Измеряется пиковая RSS память во время выполнения каждого адаптера
  - Разница в 20-30 МБ между фреймворками показывает реальные различия в потреблении памяти
- Более детальное измерение (отдельные процессы) было бы некорректно, так как:
  - Требовало бы запуска каждого фреймворка в отдельном процессе
  - Добавило бы накладные расходы на создание процессов
  - Не отражало бы реальное использование в production (где все работает в одном процессе)

**Вывод:** Измерение корректно, небольшие различия (20-30 МБ) показывают реальные различия в потреблении памяти фреймворками.

---

## 2. Alibi: быстрое выполнение и обнаружение проблем

**Замечание:** "Alibi 0.03-0.06 сек и при этом находит проблемы - возможно, но вызывает сомнения"

**Объяснение:**
- Alibi использует **IQR метод** для обнаружения выбросов (не Isolation Forest)
- IQR метод - это простой статистический расчет (Q1, Q3, IQR), который выполняется очень быстро
- Нет необходимости в обучении модели или сложных вычислениях
- Сериализация JSON происходит только для индексов выбросов, а не для всех данных
- На данных с пропусками (`pass`) Alibi находит 0 выбросов, что подтверждает корректность работы

**Вывод:** Быстрое выполнение Alibi объясняется использованием простого статистического метода (IQR), а не сложной ML-модели.

---

## 3. "Ideal" датасет с DQ=1

**Замечание:** "Ideal датасет, но DQ=1 у GX и Evidently - это выглядит как ошибка"

**Объяснение:**
- "Ideal" означает отсутствие **искусственно добавленных** проблем (пропусков, дубликатов, выбросов)
- Однако данные генерируются с использованием:
  - Стохастических процессов (Markov chains, AR процессы)
  - Случайных шумов
  - Естественной волатильности временных рядов
- GX проверяет **ожидания** (expectations), например:
  - `expect_column_values_to_be_between(column="y", min_value=0)` - может не пройти из-за случайных отрицательных значений после преобразований
  - `expect_column_values_to_be_between(column="price", min_value=0, max_value=100)` - может не пройти из-за случайных колебаний цены
- Evidently может находить проблемы в статистических характеристиках (например, изменение распределения)

**Вывод:** DQ=1 на "ideal" данных - это нормально, так как данные содержат естественную стохастичность, а не являются математически "идеальными".

---

## 4. Разные знаменатели в Data Drift (2/3, 1/2, 0/2)

**Замечание:** "Разные знаменатели - если признаки одинаковые, знаменатель должен быть одинаковым"

**Объяснение:**
- **GX, Evidently, Alibi** проверяют: `['price', 'promotion', 'y']` → знаменатель **3**
- **NannyML** проверяет только: `['price', 'promotion']` → знаменатель **2**
- Причина: NannyML не проверяет дрифт целевой переменной `y`, так как:
  - `y` используется как целевая переменная в метриках производительности
  - Проверка дрифта `y` может быть менее информативной (изменение `y` может быть ожидаемым)
  - NannyML фокусируется на дрифте признаков (features), а не целевой переменной

**Примеры из результатов:**
- `small`: GX/Evidently/Alibi = 2/3, NannyML = 1/2
- `big`: GX/Evidently/Alibi = 2/3, NannyML = 2/2
- `dr`: GX/Alibi = 3/3, Evidently = 2/3, NannyML = 0/2
- `pass`: GX/Evidently/Alibi = 2/3, NannyML = 2/2
- `ideal`: GX/Evidently/Alibi = 2/3, NannyML = 2/2

**Вывод:** Разные знаменатели - это **нормально** и отражает разные подходы фреймворков к проверке дрифта. NannyML намеренно не проверяет дрифт целевой переменной.

---

## 5. "Pass" датасет с большим количеством проблем

**Замечание:** "Pass по смыслу должен проходить проверки, но там 85 проблем качества и 3737 выбросов"

**Объяснение:**
- Название `pass` может быть **неудачным** с точки зрения интерпретации
- `pass` означает данные с **пропусками (missing values)** и **дубликатами**, а не "проходящие проверки"
- Это **тестовый сценарий** для проверки способности фреймворков обнаруживать проблемы качества данных
- **85 проблем качества** (DQ=85) - это:
  - Пропуски в колонках `ds` и `y` (2% от 20000 = 400 пропусков в `y`, плюс пропуски в `ds`)
  - Некорректные значения (отрицательные `y`)
  - Это **ожидаемый результат** - фреймворки должны находить эти проблемы

**Предложение:** Переименовать `pass` в `missing_duplicates` или `quality_issues` для ясности.

**Вывод:** Название может быть неудачным, но результаты корректны - фреймворки правильно обнаруживают проблемы качества данных.

---

## Дополнительные наблюдения

### Почему на "ideal" данных есть дрифт (DD=2/3)?
- Даже на "идеальных" данных есть естественный временной дрейф между train (первые 80%) и test (последние 20%)
- Временные ряды содержат тренды, сезонность, которые могут изменяться во времени
- Это нормальное поведение для реальных данных

### Почему NannyML находит 0/2 дрифта на "dr" данных?
- NannyML проверяет только `price` и `promotion`
- На "dr" данных дрейф может быть менее выражен в этих признаках по сравнению с `y`
- Или NannyML использует более консервативные пороги для детекции дрифта

---

## Рекомендации для улучшения

1. **Переименовать `pass`** → `missing_duplicates` или `quality_issues`
2. **Добавить в README объяснение** о том, что "ideal" не означает математически идеальные данные
3. **Уточнить в документации**, что NannyML не проверяет дрифт целевой переменной `y`
4. **Добавить комментарий** о том, что память измеряется для всего процесса Python (это корректно)

