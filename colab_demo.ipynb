{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Демонстрационный эксперимент в Google Colab (до 5 минут)\n",
        "\n",
        "### Цель\n",
        "Показать воспроизводимый протокол экспериментального сравнения фреймворков тестирования ML/данных на одной задаче прогнозирования временного ряда.\n",
        "\n",
        "### Объект сравнения\n",
        "Фреймворки:\n",
        "- Great Expectations\n",
        "- Evidently\n",
        "- Alibi Detect\n",
        "- NannyML\n",
        "\n",
        "### Входные данные\n",
        "Файлы находятся в `data/` (CSV/JSON). Используются колонки: `ds` (datetime), `y` (target), регрессоры `price`, `promotion`.\n",
        "\n",
        "### Сценарии (синтетика)\n",
        "Сценарии и “внедрённые” дефекты задаются в `generate_scenarios.py` и сохраняются в `data/`:\n",
        "- `ideal`: контрольный сценарий без внедрённых проблем.\n",
        "- `pass`: пропуски + дубликаты.\n",
        "- `dr`: дрейф + выбросы (вводятся преимущественно в тестовом хвосте).\n",
        "\n",
        "### Протокол (фиксированные правила)\n",
        "- **Time split 80/20**: данные сортируются по `ds`, тест = последние 20% строк (см. `main.py`).\n",
        "- Запуск пайплайна: обучение Prophet → прогноз `yhat` → запуск адаптеров фреймворков → сохранение артефактов.\n",
        "\n",
        "### Выходные артефакты\n",
        "Каждый запуск создаёт `reports/<run_name>/<timestamp>/...`:\n",
        "- `comparison_summary.json` (унифицированные результаты по фреймворкам)\n",
        "- `dashboard.html` (сравнительный HTML-дашборд)\n",
        "- `final_summary.json`, `model_metrics.json` и артефакты фреймворков в подпапках\n",
        "\n",
        "### Mini-table (E_test vs EF_test → итоговая таблица)\n",
        "Дополнительно строится компактная сравнительная таблица на **test split**:\n",
        "- `reports/mini_table_E_test.json` — количество и множества “внедрённых” дефектов (E_test)\n",
        "- `reports/mini_table_EF_test.csv` / `.json` — детекции из артефактов фреймворков (EF_test)\n",
        "- `reports/mini_table_final.csv` — EF_total / EF_true / FP / Recall / Precision / FalseAlarm\n",
        "- `reports/mini_table_provenance.md` — трассировка источников EF_* (какие файлы артефактов используются)\n",
        "\n",
        "Обозначения:\n",
        "- **NA** — метрика не формировалась данным средством в текущей конфигурации.\n",
        "- **—** — метрика неприменима (например, Recall при E_test=0; Precision при EF_total=0).\n",
        "\n",
        "Репозиторий: `https://github.com/alexandor09/ml-testing-frameworks-comparison/`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Делаем ячейку идемпотентной: всегда стартуем из /content,\n",
        "# чтобы повторный запуск не “углублял” cwd.\n",
        "BASE_DIR = Path(\"/content\")\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(BASE_DIR)\n",
        "\n",
        "REPO_URL = \"https://github.com/alexandor09/ml-testing-frameworks-comparison.git\"\n",
        "REPO_DIR = \"ml-testing-frameworks-comparison\"\n",
        "\n",
        "if not Path(REPO_DIR).exists():\n",
        "    subprocess.check_call([\"git\", \"clone\", REPO_URL])\n",
        "\n",
        "# Находим директорию, где лежит main.py (репо может содержать вложенную папку).\n",
        "CANDIDATES = [\n",
        "    BASE_DIR / REPO_DIR,\n",
        "    BASE_DIR / REPO_DIR / REPO_DIR,\n",
        "    BASE_DIR / REPO_DIR / REPO_DIR / REPO_DIR,\n",
        "]\n",
        "code_root = None\n",
        "for c in CANDIDATES:\n",
        "    if (c / \"main.py\").exists():\n",
        "        code_root = c\n",
        "        break\n",
        "\n",
        "if code_root is None:\n",
        "    raise FileNotFoundError(f\"Cannot find main.py in candidates: {CANDIDATES}\")\n",
        "\n",
        "os.chdir(code_root)\n",
        "print(\"cwd:\", os.getcwd())\n",
        "print(\"python:\", sys.version)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Установка зависимостей (воспроизводимый эксперимент)\n",
        "#\n",
        "# В Colab иногда возникает ошибка вида:\n",
        "#   ValueError: numpy.dtype size changed, may indicate binary incompatibility\n",
        "# Это следствие рассинхронизации бинарных колёс numpy/pandas (после частичных обновлений).\n",
        "# Надёжный порядок действий:\n",
        "# 1) выполнить эту ячейку\n",
        "# 2) Runtime → Restart runtime\n",
        "# 3) Run all (с начала)\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "INSTALL_MODE = \"full\"  # \"full\" | \"light\"\n",
        "\n",
        "def pip_install(*args: str) -> None:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *args])\n",
        "\n",
        "# Важно: no-cache-dir + force-reinstall, чтобы не подтягивались старые колёса.\n",
        "pip_install(\"--upgrade\", \"pip\")\n",
        "pip_install(\"--no-cache-dir\", \"--upgrade\", \"--force-reinstall\", \"numpy\", \"pandas\")\n",
        "\n",
        "if INSTALL_MODE == \"full\":\n",
        "    pip_install(\"--no-cache-dir\", \"--upgrade\", \"-r\", \"requirements.txt\")\n",
        "else:\n",
        "    pip_install(\n",
        "        \"--no-cache-dir\",\n",
        "        \"--upgrade\",\n",
        "        \"pandas\", \"numpy\", \"prophet\", \"great_expectations\", \"psutil\", \"plotly\", \"scikit-learn\", \"tqdm\",\n",
        "    )\n",
        "\n",
        "# Диагностика: показываем версии, не импортируя pandas (импорт может падать до Restart runtime)\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"show\", \"numpy\", \"pandas\"])\n",
        "print(\"\\nIMPORTANT: Runtime → Restart runtime, then run the notebook again from the top.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "print(\"### data/\")\n",
        "data_dir = Path(\"data\")\n",
        "for p in sorted(data_dir.glob(\"*.csv\")):\n",
        "    print(\"-\", p.as_posix())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Определения: E_test и EF_test\n",
        "\n",
        "- **E_test** — “истинные” (внедрённые) дефекты на тестовой части (последние 20% по времени).\n",
        "  - Вычисляется скриптом `scripts/count_E_test.py` из самих данных (на test split).\n",
        "- **EF_test** — дефекты, извлечённые из артефактов фреймворков на тестовой части.\n",
        "  - Вычисляется скриптом `scripts/count_EF_test.py` по `comparison_summary.json` и артефактам в `reports/run_*/<timestamp>/<framework>/...`.\n",
        "\n",
        "Сравнение выполняется в единых единицах (строки/фичи) и затем сводится в `scripts/build_mini_table.py`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# (Опционально) быстрый sanity-check: запуск одного фреймворка.\n",
        "# По умолчанию отключено, чтобы Runtime→Run all выполнял основной эксперимент (ячейка mini-table).\n",
        "RUN_QUICK_SINGLE_FRAMEWORK = False\n",
        "\n",
        "INPUT = \"data/dr.csv\"\n",
        "OUTPUT = \"reports/colab_demo_dr\"\n",
        "FRAMEWORK = \"gx\"  # gx | evidently | alibi | nannyml\n",
        "\n",
        "if RUN_QUICK_SINGLE_FRAMEWORK:\n",
        "    subprocess.check_call([\n",
        "        sys.executable, \"main.py\",\n",
        "        \"--input\", INPUT,\n",
        "        \"--format\", \"csv\",\n",
        "        \"--output\", OUTPUT,\n",
        "        \"--framework\", FRAMEWORK,\n",
        "    ])\n",
        "else:\n",
        "    print(\"Quick single-framework run is disabled (RUN_QUICK_SINGLE_FRAMEWORK=False).\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Просмотр результатов «быстрого» прогона (если он включён в предыдущей ячейке).\n",
        "run_base = Path(\"reports/colab_demo_dr\")\n",
        "\n",
        "if run_base.exists():\n",
        "    runs = sorted([p for p in run_base.glob(\"*\") if p.is_dir()])\n",
        "else:\n",
        "    runs = []\n",
        "\n",
        "if not runs:\n",
        "    print(\"No quick-run outputs found (run_base does not exist or empty).\")\n",
        "else:\n",
        "    latest = runs[-1]\n",
        "    print(\"Latest run:\", latest.as_posix())\n",
        "\n",
        "    print(\"\\n### files\")\n",
        "    for p in sorted(latest.glob(\"*\") ):\n",
        "        print(\"-\", p.name)\n",
        "\n",
        "    summary_path = latest / \"comparison_summary.json\"\n",
        "    print(\"\\n### comparison_summary.json\")\n",
        "    print(summary_path.as_posix())\n",
        "    print(json.dumps(json.loads(summary_path.read_text(encoding=\"utf-8\")), indent=2, ensure_ascii=False)[:4000])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Важно для Colab: если после pip-установок рантайм не был перезапущен,\n",
        "# возможна бинарная несовместимость numpy/pandas.\n",
        "try:\n",
        "    import pandas as pd\n",
        "except Exception as e:\n",
        "    if \"numpy.dtype size changed\" in str(e):\n",
        "        raise RuntimeError(\n",
        "            \"Binary incompatibility between numpy and pandas. \"\n",
        "            \"Fix: Runtime → Restart runtime, then run the notebook again from the top.\"\n",
        "        ) from e\n",
        "    raise\n",
        "\n",
        "print(\"pandas:\", pd.__version__)\n",
        "\n",
        "# Полный эксперимент: прогон 3 сценариев (ideal/pass/dr) всеми 4 фреймворками\n",
        "# и последующее построение mini-table.\n",
        "SCENARIOS = [\n",
        "    (\"ideal\", \"data/ideal.csv\", \"reports/run_ideal\"),\n",
        "    (\"pass\",  \"data/pass.csv\",  \"reports/run_pass\"),\n",
        "    (\"dr\",    \"data/dr.csv\",    \"reports/run_dr\"),\n",
        "]\n",
        "\n",
        "for name, inp, out in SCENARIOS:\n",
        "    print(f\"\\n=== RUN: {name} ===\")\n",
        "    subprocess.check_call([\n",
        "        sys.executable, \"main.py\",\n",
        "        \"--input\", inp,\n",
        "        \"--format\", \"csv\",\n",
        "        \"--output\", out,\n",
        "    ])\n",
        "\n",
        "# Извлекаем сводные таблицы сравнения из comparison_summary.json\n",
        "\n",
        "def _latest_run_dir(parent: str) -> Path:\n",
        "    p = Path(parent)\n",
        "    runs = sorted([x for x in p.glob(\"*\") if x.is_dir()])\n",
        "    if not runs:\n",
        "        raise FileNotFoundError(f\"No runs found in {parent}\")\n",
        "    return runs[-1]\n",
        "\n",
        "\n",
        "def _load_summary(run_dir: Path) -> dict:\n",
        "    return json.loads((run_dir / \"comparison_summary.json\").read_text(encoding=\"utf-8\"))\n",
        "\n",
        "\n",
        "def _summary_df(summary: dict, scenario: str) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for fw, res in (summary or {}).items():\n",
        "        rows.append(\n",
        "            {\n",
        "                \"scenario\": scenario,\n",
        "                \"framework\": fw,\n",
        "                \"time_sec\": float(res.get(\"execution_time_sec\", 0.0) or 0.0),\n",
        "                \"peak_rss_mb\": float(res.get(\"memory_peak_mb\", 0.0) or 0.0),\n",
        "                \"issues_detected\": int(res.get(\"issues_detected\", 0) or 0),\n",
        "                \"coverage\": float(res.get(\"coverage_score\", 0.0) or 0.0),\n",
        "                \"artifacts_n\": int(len(res.get(\"artifacts\", []) or [])),\n",
        "                \"data_quality\": bool((res.get(\"checks_performed\", {}) or {}).get(\"data_quality\", False)),\n",
        "                \"data_drift\": bool((res.get(\"checks_performed\", {}) or {}).get(\"data_drift\", False)),\n",
        "                \"outliers\": bool((res.get(\"checks_performed\", {}) or {}).get(\"outliers\", False)),\n",
        "                \"model_perf\": bool((res.get(\"checks_performed\", {}) or {}).get(\"model_performance\", False)),\n",
        "            }\n",
        "        )\n",
        "    df = pd.DataFrame(rows)\n",
        "    if len(df) == 0:\n",
        "        return df\n",
        "    return df.sort_values([\"scenario\", \"framework\"]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"\\n=== COMPARISON TABLES (from comparison_summary.json) ===\")\n",
        "all_tables = []\n",
        "for scen, _inp, out_parent in SCENARIOS:\n",
        "    run_dir = _latest_run_dir(out_parent)\n",
        "    summary = _load_summary(run_dir)\n",
        "    df = _summary_df(summary, scenario=scen)\n",
        "    all_tables.append(df)\n",
        "    print(f\"\\nScenario: {scen}\")\n",
        "    display(df[[\"framework\", \"time_sec\", \"peak_rss_mb\", \"issues_detected\", \"coverage\", \"artifacts_n\"]])\n",
        "\n",
        "# Объединённая таблица (все сценарии вместе)\n",
        "df_all = pd.concat(all_tables, ignore_index=True)\n",
        "print(\"\\nAll scenarios (stacked):\")\n",
        "display(df_all)\n",
        "\n",
        "# Построение mini-table (E_test, EF_test, final)\n",
        "print(\"\\n=== MINI-TABLE PIPELINE ===\")\n",
        "subprocess.check_call([sys.executable, \"scripts/count_E_test.py\", \"--inputs\", \"data/pass.csv\", \"data/dr.csv\", \"data/ideal.csv\"])\n",
        "subprocess.check_call([sys.executable, \"scripts/count_EF_test.py\"])\n",
        "subprocess.check_call([sys.executable, \"scripts/build_mini_table.py\"])\n",
        "\n",
        "print(\"\\nSaved:\")\n",
        "print(\"- reports/mini_table_E_test.json\")\n",
        "print(\"- reports/mini_table_EF_test.json\")\n",
        "print(\"- reports/mini_table_EF_test.csv\")\n",
        "print(\"- reports/mini_table_final.csv\")\n",
        "print(\"- reports/mini_table_provenance.md\")\n",
        "\n",
        "print(\"\\nPreview: reports/mini_table_final.csv\")\n",
        "df_final = pd.read_csv(\"reports/mini_table_final.csv\")\n",
        "display(df_final)\n",
        "\n",
        "# (Опционально) скачивание dashboard.html для сценария dr\n",
        "try:\n",
        "    from google.colab import files  # type: ignore\n",
        "\n",
        "    dr_run_dir = _latest_run_dir(\"reports/run_dr\")\n",
        "    dash_path = dr_run_dir / \"dashboard.html\"\n",
        "    if dash_path.exists():\n",
        "        print(f\"\\nDownloading dashboard: {dash_path.as_posix()}\")\n",
        "        files.download(dash_path.as_posix())\n",
        "except Exception:\n",
        "    # Вне Colab этот блок можно игнорировать\n",
        "    pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Дополнительный запуск (один сценарий)\n",
        "\n",
        "Основной эксперимент (прогоны `ideal/pass/dr` + сравнение + mini-table) выполняется в ячейке выше.\n",
        "\n",
        "Ниже оставлена дополнительная ячейка для запуска **одного** сценария (например, только `dr`) и получения `dashboard.html`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Дополнительный запуск одного сценария (отдельная папка)\n",
        "# По умолчанию отключено, чтобы не тратить время при Runtime→Run all.\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "RUN_EXTRA_SINGLE_SCENARIO = False\n",
        "\n",
        "INPUT_FULL = \"data/dr.csv\"\n",
        "OUTPUT_FULL = \"reports/colab_demo_full\"\n",
        "\n",
        "if RUN_EXTRA_SINGLE_SCENARIO:\n",
        "    subprocess.check_call([\n",
        "        sys.executable, \"main.py\",\n",
        "        \"--input\", INPUT_FULL,\n",
        "        \"--format\", \"csv\",\n",
        "        \"--output\", OUTPUT_FULL,\n",
        "    ])\n",
        "\n",
        "    run_base = Path(OUTPUT_FULL)\n",
        "    runs = sorted([p for p in run_base.glob(\"*\") if p.is_dir()])\n",
        "    latest = runs[-1]\n",
        "    print(\"Latest full run:\", latest.as_posix())\n",
        "\n",
        "    print(\"\\nDashboard:\", (latest / \"dashboard.html\").as_posix())\n",
        "    print(\"Final summary:\", (latest / \"final_summary.json\").as_posix())\n",
        "\n",
        "    # В Colab можно скачать HTML и открыть локально в браузере.\n",
        "    try:\n",
        "        from google.colab import files  # type: ignore\n",
        "\n",
        "        files.download((latest / \"dashboard.html\").as_posix())\n",
        "    except Exception:\n",
        "        pass\n",
        "else:\n",
        "    print(\"Extra single-scenario run is disabled (RUN_EXTRA_SINGLE_SCENARIO=False).\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mini-table (E_test vs EF_total/EF_true/FP + FalseAlarm)\n",
        "\n",
        "Требует `INSTALL_MODE = \"full\"` (потому что нужны артефакты всех 4 фреймворков).\n",
        "\n",
        "Пайплайн:\n",
        "- запускаем `ideal`, `pass`, `dr` → артефакты в `reports/run_*/<timestamp>/...`\n",
        "- считаем `E_test`, `EF_test`, собираем финальную таблицу\n",
        "\n",
        "Важно:\n",
        "- В итоговой таблице есть `n_test` (контекст размера теста).\n",
        "- **NA** = метрика не формировалась данным средством в текущей конфигурации.\n",
        "- **—** = метрика неприменима (например, Recall при `E_test=0`; Precision при `EF_total=0` или при `E_test=0`).\n",
        "- `FalseAlarm` выводится только когда `E_test=0`:\n",
        "  - для строковых дефектов: `EF_total / n_test`\n",
        "  - для `drift_features`: `EF_total / 3`\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
